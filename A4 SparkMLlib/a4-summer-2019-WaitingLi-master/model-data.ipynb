{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Predictive Model(s)\n",
    "\n",
    "In this workbook, you will read the merged dataset you created previously and you will create transformer, estimators and pipelines to build a binary classification model to predict wether a trip has a tip or not.\n",
    "\n",
    "## Instructions:\n",
    "\n",
    "1. Read in your merged dataset\n",
    "2. Use transformes and encoders to perform feature engineering\n",
    "3. Split into training and testing\n",
    "4. Build `LogisticRegression` model(s) and train them using pipelines\n",
    "5. Evaluate the performance of the model(s) using `BinaryClassificationMetrics`\n",
    "\n",
    "You are welcome to add as many cells as you need below up until the next section. **You must include comments in your code.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"assign-4-model\").getOrCreate()\n",
    "\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import Binarizer\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, IndexToString, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline, Model\n",
    "\n",
    "from pyspark.sql.functions import to_timestamp, year, month, dayofweek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.parquet('s3://weiteng-li/merged_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- rate_code: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_time_in_secs: float (nullable = true)\n",
      " |-- trip_distance: float (nullable = true)\n",
      " |-- pickup_longitude: float (nullable = true)\n",
      " |-- pickup_latitude: float (nullable = true)\n",
      " |-- dropoff_longitude: float (nullable = true)\n",
      " |-- dropoff_latitude: float (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: float (nullable = true)\n",
      " |-- surcharge: float (nullable = true)\n",
      " |-- mta_tax: float (nullable = true)\n",
      " |-- tip_amount: float (nullable = true)\n",
      " |-- tolls_amount: float (nullable = true)\n",
      " |-- total_amount: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert 'tip_amount' to a binary feature\n",
    "data = data.withColumn('tip_amount',data['tip_amount'].cast('double'))\n",
    "binarizer = Binarizer(threshold=0.0, inputCol=\"tip_amount\", outputCol=\"labels\")\n",
    "data = binarizer.transform(data)\n",
    "stringIndexer_label = StringIndexer(inputCol=\"labels\", outputCol=\"target\")\n",
    "label_fit = StringIndexer(inputCol=\"labels\", outputCol=\"target\").fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build up datetime features\n",
    "data = data.withColumn('dropoff_year', year('dropoff_datetime'))\n",
    "data = data.withColumn('dropoff_month', month('dropoff_datetime'))\n",
    "data = data.withColumn('dropoff_dayofweek', dayofweek('dropoff_datetime'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stringIndexer\n",
    "payment_type = StringIndexer(inputCol=\"payment_type\", outputCol=\"payment_type_IDX\").setHandleInvalid(\"keep\")\n",
    "store_and_fwd_flag = StringIndexer(inputCol=\"store_and_fwd_flag\", outputCol=\"store_and_fwd_flag_IDX\").setHandleInvalid(\"keep\")\n",
    "dropoff_year = StringIndexer(inputCol=\"dropoff_year\", outputCol=\"dropoff_year_IDX\").setHandleInvalid(\"keep\")\n",
    "dropoff_month = StringIndexer(inputCol=\"dropoff_month\", outputCol=\"dropoff_month_IDX\").setHandleInvalid(\"keep\")\n",
    "dropoff_dayofweek = StringIndexer(inputCol=\"dropoff_dayofweek\", outputCol=\"dropoff_dayofweek_IDX\").setHandleInvalid(\"keep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "iput_cols =   [ \"store_and_fwd_flag_IDX\",\n",
    "               \"payment_type_IDX\", \n",
    "               \n",
    "               \"trip_time_in_secs\",\n",
    "               \"trip_distance\", \n",
    "               \"fare_amount\", \n",
    "               \"surcharge\" , \n",
    "               \"tolls_amount\",\n",
    "               \"total_amount\", \n",
    "               \"passenger_count\",\n",
    "               \n",
    "               \"dropoff_year_IDX\",\n",
    "               \"dropoff_month_IDX\",\n",
    "               \"dropoff_dayofweek_IDX\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler_features = VectorAssembler(inputCols=iput_cols, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Training Set & Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training records: 147202464\n",
      "Number of testing records : 25974649\n"
     ]
    }
   ],
   "source": [
    "train_data,test_data = data.randomSplit(weights = [0.85, 0.15], seed=2333)\n",
    "print(\"Number of training records: \" + str(train_data.count()))\n",
    "print(\"Number of testing records : \" + str(test_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(labelCol=\"labels\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[payment_type,\n",
    "                            store_and_fwd_flag, \n",
    "                            dropoff_year,\n",
    "                            dropoff_month,\n",
    "                            dropoff_dayofweek,\n",
    "                            vectorAssembler_features,\n",
    "                            logit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test_data)\n",
    "predictions_target_table = predictions.select('prediction','labels').rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9980430071208847"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "metrics = BinaryClassificationMetrics(predictions_target_table)\n",
    "metrics.areaUnderROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the following cells, please provide the requested code and output. Do not change the order and/or structure of the cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, print the Area Under the Curve (AUC) for your binary classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9980430071208847"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.areaUnderROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, provide the code that saves your model your S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit.save('s3://weiteng-li/logit/')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
